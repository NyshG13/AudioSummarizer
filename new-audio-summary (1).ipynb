{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12302259,"sourceType":"datasetVersion","datasetId":7754166},{"sourceId":12302404,"sourceType":"datasetVersion","datasetId":7754269}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install dependencies","metadata":{}},{"cell_type":"code","source":"!pip install -q demucs\n!pip install -q transformers torchaudio librosa accelerate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from demucs.apply import apply_model\nfrom demucs.pretrained import get_model\nfrom demucs.audio import AudioFile\nimport torchaudio\nimport torch\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Noise reduction model","metadata":{}},{"cell_type":"code","source":"model = get_model(name='htdemucs')\n# model.cpu()\nmodel.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_audio_path = \"/kaggle/input/whisper-test-5/audio test timeless.unknown\"\noutput_dir = \"/kaggle/working/denoised\"\nos.makedirs(output_dir, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"source = AudioFile(input_audio_path)\nref = source.read(streams=0, channels=1)\nwav = ref[0]\nsample_rate = source.samplerate()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Ensure wav is a tensor with shape (1, T)\nif not isinstance(wav, torch.Tensor):\n    wav = torch.tensor(wav)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if wav.ndim == 1:\n    wav = wav.unsqueeze(0)\nprint(wav.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if wav.shape[0] == 1:\n    wav = torch.cat([wav, wav], dim=0)\nprint(wav.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wav = wav.unsqueeze(0).float()\nprint(wav.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    sources = apply_model(model, wav)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocals = sources[0][3]\nvocals_path = os.path.join(output_dir, \"vocals.wav\")\ntorchaudio.save(vocals_path, vocals.cpu(), sample_rate)\n\nprint(\"âœ… Denoising complete. Saved vocals to:\", vocals_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Audio\n\nAudio(vocals_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Speech to Text ","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\nasr = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\", return_timestamps=True)\nresult = asr(\"/kaggle/working/denoised/vocals.wav\")\nprint(result[\"text\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transcription = result[\"text\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transcript = result['text']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Summarization","metadata":{}},{"cell_type":"code","source":"from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n\n# Load model and tokenizer\nmodel_name = \"google/pegasus-large\"\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\npegasus = PegasusForConditionalGeneration.from_pretrained(model_name)\n\ndef summarize_text(text):\n    inputs = tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n    summary_ids = pegasus.generate(inputs[\"input_ids\"], max_length=150, min_length=15, length_penalty=2.0, num_beams=4)\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary\n\n# Summarize transcription\nsummary = summarize_text(transcription)\nprint(\"ðŸ“Œ Summary:\\n\", summary)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\nimport textwrap\n\n# Load summarization pipeline\nsummarizer = pipeline(\"summarization\", model=\"google/pegasus-large\", tokenizer=\"google/pegasus-large\")\n\n# Split long text into chunks (1000 tokens ~ 3000-3500 characters)\ndef split_into_chunks(text, max_chunk_size=3500):\n    return textwrap.wrap(text, width=max_chunk_size, break_long_words=False)\n\n# Assume 'transcript' contains your full Whisper output\nchunks = split_into_chunks(transcript)\n\n# Summarize each chunk\nchunk_summaries = [summarizer(chunk, max_length=120, min_length=30, do_sample=False)[0][\"summary_text\"] for chunk in chunks]\n\n# Optional: summarize all summaries into one\nfinal_summary = summarizer(\" \".join(chunk_summaries), max_length=150, min_length=50, do_sample=False)[0][\"summary_text\"]\n\nprint(\"Final Summary:\\n\", final_summary)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}